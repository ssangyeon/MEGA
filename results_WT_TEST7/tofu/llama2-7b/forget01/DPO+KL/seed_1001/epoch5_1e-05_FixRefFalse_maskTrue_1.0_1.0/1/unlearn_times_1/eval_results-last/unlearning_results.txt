Real Authors ROUGE: 0.9263333333333332
Real Authors Probability: 0.4886708917265575
Real Authors Truth Ratio: 0.6172821497124106
Real Authors Token Entropy: 0.9862768487674469
Real Authors Cosine Similarity: 0.959611639380455
Real Authors Entailment Score: 0.92
Real World ROUGE: 0.8717948717948718
Real World Probability: 0.45703177716719356
Real World Truth Ratio: 0.5486332770643546
Real World Token Entropy: 0.966136183693524
Real World Cosine Similarity: 0.9515749623632839
Real World Entailment Score: 0.7692307692307693
Retain ROUGE: 0.8864977575792382
Retain Probability: 0.9657505741751773
Retain Truth Ratio: 0.4499652986374011
Retain Token Entropy: 0.9735078931422197
Retain Cosine Similarity: 0.9576500750084718
Retain Entailment Score: 0.9433333333333334
Forget ROUGE: 0.3798053112881969
Forget Probability: 0.8412069244618035
Forget Truth Ratio: 0.3998151944965448
Forget Token Entropy: 0.9756189078602467
Forget Cosine Similarity: 0.621110781817697
Forget Entailment Score: 0.15
Model Utility Retain: 0.7981484497023964
Model Utility: 0.7515658891780052
Forget Efficacy: 0.5216123575871515
Model Utility Retain_base: 0.6840113573418379
Model Utility_base: 0.6298291051590706
Forget Efficacy_base: 0.45972418991781827
split: forget01
forget_loss: DPO+KL
forget_coeff: 1.0
regularization_coeff: 1.0
learning_rate: 1e-05
epochs: 5
fix_ref_model: False
mask: True
unlearn_step: last
task_id: 1
unlearn_times: 1
